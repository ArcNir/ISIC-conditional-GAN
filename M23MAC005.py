# -*- coding: utf-8 -*-
"""testvae

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/testvae-a729764a-e686-4907-afb9-7913954a7347.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240502/auto/storage/goog4_request%26X-Goog-Date%3D20240502T132039Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D48e1cb2944e8fe1f9e722ddecb2c41b49ca83bf8c7c4bcf5f7011f4bbad92e79d43fa0e84935f89a408bf0c44a884154d2da4dafec18750e5ec86c4a9cd6df4be729e572218d80df666e5ed7710486ce1964a88dbcdcb449926f2ddb4f02b925190059a91d4d57b35d85205481ef20761af6bf85f3bd359701bbe426f86845faf6a17628217bb19855ce7d2af53217e6d69a256c18cc47a8a3862cdfc4837180f700f81a826c26a802a984ce1c1510bee82852df7c10bf82b9e3e645876fa97632e763128e3a853c6bbc686c330d8b2186e3ba9a9c1b42fca227e941d513295c17476fe2cab0dde0b57c40369a0a6419d9447a9ed21b2f48011f854eebdb1007
"""

# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# THEN FEEL FREE TO DELETE THIS CELL.
# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# NOTEBOOK.

import os
import sys
from tempfile import NamedTemporaryFile
from urllib.request import urlopen
from urllib.parse import unquote, urlparse
from urllib.error import HTTPError
from zipfile import ZipFile
import tarfile
import shutil

CHUNK_SIZE = 40960
DATA_SOURCE_MAPPING = 'isic-skin-lesion-dataset-2016:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4856083%2F8197887%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240502%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240502T132039Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D79bde9899a12920aba33dc37943e3b25147fbc215182a686b1967191c9c8c7749ab6abee9f5a0f11442a5f984001f57264869ac7917130255e55c224ce228fcec1b67bf6f51a1b407d2d0d85e978f38b42df816c1acd4cb86680856a3a4888fc129b2d65e36b5194498f4286bfdc9d121b215fea733b789bc1351d4fa33dc605040e56cab4a06a474ddd8ecac2bb4689bc9825cf124b4d7a0dbfd27ca806314aa1d7978fbc3e21cdecf9d72fd39cef8b90c2185bc2786042e121e1819c58800a6b633ada48bcb1bbfe0b058dda3a1e9c3140b2b3378a95ece5c06cf2d89d49665fd263db9797fa85a049d42efaa7c4548cf62ab6120c6000620151be35dbc5b3,vq-weights:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4918481%2F8281761%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240502%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240502T132039Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D5f113431669e24fc98596c12472bcb569d60659f7d21a010ddad293db2bb67198b6ad320c0798a15edb3b8e6687127a4ed47695a55239c8fb040c635773a22acc70db354dde9d809794ec8deb58a86e2bd865fb3fad806a1ab8a883027369773045d3e1d3a8efc3e3d8e7bc13e3e871a67f8e148f4b235584e2b3d72394582a6c72e4cb14c3c2e7bbe5193afa94283cf31a9094d4ca2621ae5f3c23458b801b15134dd454b07786f72ff2ab64b243f57b5c11651b411b94e584ec17c0101c3999d8677b783f7d84ed7ec427cdc798fe7a696894bf02ac052bb060e932f97a80bc7d28085c7b785840ad425d88504efd651769f33f3801e0f59d8cc41d0f09fab,testfile:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4920040%2F8283843%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240502%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240502T132039Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D69d6436f53cb26b4bcabb893e907df8bbf23e7174f749e8e06e265ec84f906d36ed3a8bddcd6fc17a7c11946373e90ee426ae4fac55ce13b6823a045dbf9e251d0ef854a9f5bf8d90da8173d4694a40784cdc54dcd2a5b8c63efd5c56c806dd8483ed54e9984ad7443e814402f70fedeb211a2b5e3b5984a0a5dc2c5c26c06a92de4547bd93eebffcab8b8cdb71e4f5a10f7aaa38385836be7cef3ed1f00ee9b6ae7fb37bf247f5e4f4e8dfad0c1bd47939d0c7e759456e7860f4af9054e7482beeb077d18dbd9de3bb72692b06ac5917faa96eb5513f3dec395215202ce43ee77786954292d5b465dce4d2e8f8f709312537127c3aecb06c6a016ea030ba39e,pixelcnn:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F4920080%2F8283890%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240502%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240502T132039Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D354db0c8929b7f1d5c616c35bf964f39c98093c9b5321acf7e5c1147b32c0d4a49bf8e8d93c76c7ba87aedab431db91d613bd6896c159196b6ef325c1f3cc88eb744f5f0c97295865696a3bff5da62f208cceda1449337fdea7cb12925674ea92f39dd06ca18188c3bbbded9ee530c3d40dac5169d00ed9fbf0b51ac1ae195a085bf06575cecd6ed05e32928bbe58b462b29b7f96390423e88413b2c6abfecd31f8c3ed4bf134042430a843242d2f9aa7aae9f693f256f21b3d5633e0cb6e56ed94ec8e966fdc7e6c932b7e266f3cd474253d14f844e60d0bec99b8bb3e99c195432f3fa4b7a3818d98fdfa0b1059e1ed4a15d919a157df725d9a07151049818'

KAGGLE_INPUT_PATH='/kaggle/input'
KAGGLE_WORKING_PATH='/kaggle/working'
KAGGLE_SYMLINK='kaggle'

!umount /kaggle/input/ 2> /dev/null
shutil.rmtree('/kaggle/input', ignore_errors=True)
os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

try:
  os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
except FileExistsError:
  pass
try:
  os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
except FileExistsError:
  pass

for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
    directory, download_url_encoded = data_source_mapping.split(':')
    download_url = unquote(download_url_encoded)
    filename = urlparse(download_url).path
    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
    try:
        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
            total_length = fileres.headers['content-length']
            print(f'Downloading {directory}, {total_length} bytes compressed')
            dl = 0
            data = fileres.read(CHUNK_SIZE)
            while len(data) > 0:
                dl += len(data)
                tfile.write(data)
                done = int(50 * dl / int(total_length))
                sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
                sys.stdout.flush()
                data = fileres.read(CHUNK_SIZE)
            if filename.endswith('.zip'):
              with ZipFile(tfile) as zfile:
                zfile.extractall(destination_path)
            else:
              with tarfile.open(tfile.name) as tarfile:
                tarfile.extractall(destination_path)
            print(f'\nDownloaded and uncompressed: {directory}')
    except HTTPError as e:
        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
        continue
    except OSError as e:
        print(f'Failed to load {download_url} to path {destination_path}')
        continue

print('Data source import complete.')

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import os
import random


import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions.normal import Normal


import torchvision.datasets as datasets
import torchvision.transforms as transforms
from torchvision.utils import make_grid
from torch.utils.data import Dataset, DataLoader, random_split

import torchvision
from PIL import Image

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

parameters = {
    "n_epochs": 100,
    "batch_size": 256,
    "lr": 1e-3,
    "b1": 0.0,
    "b2": 0.9,
    "latent_dim": 100,
    "embed":100,
    "features":64,
    "n_classes": 7,
    "img_size":64,
    "channels": 3,
    "sample_interval": 10,
    "commitment_cost": 0.25,
    "num_embeddings": 512,
    "embedding_dim": 64,
    "num_residual_layers": 2,
    "num_hiddens": 128,
    "num_residual_hiddens": 32,
    "img_dim":8,
    "n_layers":15,
    "n_classes":7,
    "decay": 0.99
}

"""# Dataset

"""

class CustomDataset(Dataset):
    def __init__(self, img_folder, csv_file, transform=None):
        self.img_folder = img_folder
        self.csv_file = csv_file
        self.transform = transform
        self.data_info = pd.read_csv(csv_file)
    def __len__(self):
        return len(self.data_info)
    def __getitem__(self, index):
        img_name = os.path.join(self.img_folder, self.data_info.iloc[index, 0] + '.jpg') # Add file extension
        image = Image.open(img_name)
        if self.transform is not None:
            image = self.transform(image)
        label = self.data_info.iloc[index, 1:].values.astype(np.float32) # Convert labels to float32
        label = np.argmax(label)
        return image, label

transform = transforms.Compose([
    transforms.Resize((32, 32)),
    transforms.RandomHorizontalFlip(),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),
    transforms.ToTensor(),
    transforms.Normalize((0.5,), (0.5,))
])
dataset = CustomDataset(csv_file='/kaggle/input/isic-skin-lesion-dataset-2016/Train-20240422T183231Z-002/Train/Train_labels.csv', img_folder="/kaggle/input/isic-skin-lesion-dataset-2016/Train_data-001/Train_data", transform=transform)

batch_size = parameters["batch_size"]
data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

train_size = int(0.7 * len(data_loader.dataset))
val_size = int(0.15 * len(data_loader.dataset))
test_size = len(data_loader.dataset) - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(data_loader.dataset, [train_size, val_size, test_size])

train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=7, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=7, shuffle=False)

total_variance = 0.0
total_samples = 0

for images,_ in train_loader:
    batch_size = images.size(0)
    print(batch_size)
    total_samples += batch_size
    images_np = images.numpy() / 255.0
    batch_variance = np.var(images_np)
    total_variance += batch_variance * batch_size

data_variance = total_variance / total_samples

"""## VQ-VAE

"""

class VectorQuantizer(nn.Module):
    def __init__(self, num_embeddings, embedding_dim, commitment_cost, decay, epsilon=1e-5):
        super().__init__()

        self.embedding_dim = embedding_dim
        self.num_embeddings = num_embeddings

        self.embedding = nn.Embedding(self.num_embeddings, self.embedding_dim)
        self.embedding.weight.data.normal_()
        self.commitment_cost = commitment_cost

        self.register_buffer('ema_cluster_size', torch.zeros(num_embeddings))
        self.ema_w = nn.Parameter(torch.Tensor(num_embeddings, self.embedding_dim))
        self.ema_w.data.normal_()

        self.decay = decay
        self.epsilon = epsilon

    def forward(self, inputs):
        inputs = inputs.permute(0, 2, 3, 1).contiguous()
        input_shape = inputs.shape

        flat_input = inputs.view(-1, self.embedding_dim)

        distances = (torch.sum(flat_input**2, dim=1, keepdim=True)
                     + torch.sum(self.embedding.weight**2, dim=1)
                     - 2 * torch.matmul(flat_input, self.embedding.weight.t()))

        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)
        encodings = torch.zeros(encoding_indices.shape[0], self.num_embeddings, device=inputs.device)
        encodings.scatter_(1, encoding_indices, 1)

        quantized = torch.matmul(encodings, self.embedding.weight).view(input_shape)

        if self.training:
            self.ema_cluster_size = self.ema_cluster_size * self.decay + \
                                    (1 - self.decay) * torch.sum(encodings, 0)

            n = torch.sum(self.ema_cluster_size.data)
            self.ema_cluster_size = (
                (self.ema_cluster_size + self.epsilon)
                / (n + self.num_embeddings * self.epsilon) * n)

            dw = torch.matmul(encodings.t(), flat_input)
            self.ema_w = nn.Parameter(self.ema_w * self.decay + (1 - self.decay) * dw)

            self.embedding.weight = nn.Parameter(self.ema_w / self.ema_cluster_size.unsqueeze(1))

        e_latent_loss = F.mse_loss(quantized.detach(), inputs)
        loss = self.commitment_cost * e_latent_loss

        quantized = inputs + (quantized - inputs).detach()
        avg_probs = torch.mean(encodings, dim=0)
        quantized = quantized.permute(0, 3, 1, 2).contiguous()
        return loss, quantized, encodings, encoding_indices

class ResidualBlock(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_hiddens, num_residual_layers):
        super().__init__()
        self.num_residual_layers = num_residual_layers
        self.layers = nn.ModuleList([nn.Sequential(nn.ReLU(True),nn.Conv2d(in_channels, num_residual_hiddens, 3, 1, 1, bias=False),nn.ReLU(True),nn.Conv2d(num_residual_hiddens, num_hiddens, 1, 1, bias=False)) for _ in range(num_residual_layers)])

    def forward(self, x):
        for layer in self.layers:
            x = x + layer(x)
            rs = F.relu(x)
        return rs

class Encoder(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super().__init__()

        self.conv_1 = nn.Conv2d(in_channels, num_hiddens//2, 4, 2, 1)
        self.conv_2 = nn.Conv2d(num_hiddens//2, num_hiddens, 4, 2, 1)
        self.conv_3 = nn.Conv2d(num_hiddens, num_hiddens, 3, 1, 1)
        self.residual_block = ResidualBlock(num_hiddens, num_hiddens, num_residual_hiddens, num_residual_layers)

    def forward(self, inputs):
        x = self.conv_1(inputs)
        x = F.relu(x)
        x = self.conv_2(x)
        x = F.relu(x)
        x = self.conv_3(x)
        return self.residual_block(x)

class Decoder(nn.Module):
    def __init__(self, in_channels, num_hiddens, num_residual_layers, num_residual_hiddens):
        super().__init__()
        self.conv_1 = nn.Conv2d(in_channels, num_hiddens, 3, 1, 1)
        self.residual_block = ResidualBlock(num_hiddens, num_hiddens, num_residual_hiddens, num_residual_layers)
        self.conv_trans_1 = nn.ConvTranspose2d(num_hiddens, num_hiddens//2, 4, 2, 1)
        self.conv_trans_2 = nn.ConvTranspose2d(num_hiddens//2, 3, 4, 2, 1)

    def forward(self, inputs):
        x = self.conv_1(inputs)
        x = self.residual_block(x)
        x = self.conv_trans_1(x)
        x = F.relu(x)
        return self.conv_trans_2(x)

class Model(nn.Module):
    def __init__(self, num_hiddens, num_residual_layers, num_residual_hiddens,
                 num_embeddings, embedding_dim, commitment_cost, decay=0):
        super().__init__()

        self.encoder = Encoder(3, num_hiddens, num_residual_layers, num_residual_hiddens)
        self.pre_vq_conv = nn.Conv2d(num_hiddens, embedding_dim, 1, 1)
        self.vq_vae = VectorQuantizer(num_embeddings, embedding_dim, commitment_cost, decay)
        self.decoder = Decoder(embedding_dim, num_hiddens, num_residual_layers, num_residual_hiddens)

    def forward(self, x):
        z = self.encoder(x)
        z = self.pre_vq_conv(z)
        loss, quantized, _,_ = self.vq_vae(z)
        x_recon = self.decoder(quantized)

        return loss, x_recon, perplexity

model = Model(parameters["num_hiddens"], parameters["num_residual_layers"], parameters["num_residual_hiddens"],
              parameters["num_embeddings"], parameters["embedding_dim"], parameters["commitment_cost"],parameters["decay"]).to(device)

optimizer = optim.Adam(model.parameters(), lr=parameters["lr"], amsgrad=False)

model.train()
train_res_recon_error = []

for i in range(parameters["n_epochs"]):
    for imgs,_ in train_loader:
        imgs = imgs.to(device)
        optimizer.zero_grad()

        vq_loss, data_recon, _ = model(imgs)
        recon_error = F.mse_loss(data_recon, imgs) / data_variance
        loss = recon_error + vq_loss
        loss.backward()

        optimizer.step()

        train_res_recon_error.append(recon_error.item())

    print('Epoch: %d' % (i+1))
    print('recon_error: %.3f' % np.mean(train_res_recon_error[-len(train_loader):]))
    print()

model.load_state_dict(torch.load('/kaggle/input/vq-weights/model_weights_2000_epochs.pth'))
model.eval()

def show(img):
    npimg = img.numpy()
    fig = plt.imshow(np.transpose(npimg, (1,2,0)), interpolation='nearest')
    fig.axes.get_xaxis().set_visible(False)
    fig.axes.get_yaxis().set_visible(False)

valid_originals,_ = next(iter(val_loader))
valid_originals = valid_originals.to(device)

vq_output_eval = model.pre_vq_conv(model.encoder(valid_originals))
_, valid_quantize, _, _ = model.vq_vae(vq_output_eval)
valid_reconstructions = model.decoder(valid_quantize)

show(make_grid(valid_reconstructions.cpu().data)+0.5, )

show(make_grid(valid_originals.cpu()+0.5))

"""## PixelCNN Prior"""

def weights_init(m):
    if isinstance(m, nn.Conv2d):
        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        if m.bias is not None:
            nn.init.constant_(m.bias, 0)
    elif isinstance(m, nn.Embedding):
        nn.init.normal_(m.weight, mean=0, std=0.01)

class PixelCNN(nn.Module):
    def __init__(self, input_dim, dim, n_layers, n_classes):
        super().__init__()
        self.dim = dim
        self.embedding = nn.Embedding(input_dim, dim)
        self.layers = nn.ModuleList([nn.Conv2d(dim, dim, kernel_size=7, padding=3) for _ in range(n_layers)])
        self.output_conv = nn.Conv2d(dim, input_dim, kernel_size=1)

        self.mask = torch.ones(1, 1, 7, 7, device='cuda')  # Assuming CUDA device

        self.apply(weights_init)

    def masked_convolution(self, x, layer_idx):
        mask = self.mask.clone()
        if layer_idx == 0:
            mask[:, :, 3, 3:].fill_(0)
            mask[:, :, 3:, :].fill_(0)
        else:
            mask[:, :, 3, 3:].fill_(1)
            mask[:, :, 3:, :].fill_(1)

        masked_weight = self.layers[layer_idx].weight * mask
        out = F.conv2d(x, masked_weight, self.layers[layer_idx].bias, padding=3)
        return out

    def forward(self, x, label):
        shp = x.size() + (-1,)
        x = self.embedding(x.view(-1)).view(shp).permute(0, 3, 1, 2).to('cuda')  # Assuming input is on CUDA
        for i, layer in enumerate(self.layers):
            x = F.relu(self.masked_convolution(x, i))
        return self.output_conv(x)

    def generate(self, label, shape=(8, 8), batch_size=64):
        device = 'cuda'
        x = torch.zeros((batch_size, *shape), dtype=torch.int64, device=device)
        for i in range(shape[0]):
            for j in range(shape[1]):
                logits = self.forward(x, label)
                probs = F.softmax(logits[:, :, i, j], -1)
                x[:, i, j] = probs.multinomial(1).squeeze()
        return x

prior = PixelCNN(parameters["num_embeddings"], parameters["img_dim"]**2, parameters["n_layers"],parameters["n_classes"]).to(device)
criterion = nn.CrossEntropyLoss().cuda()
opt = torch.optim.Adam(prior.parameters(), lr=3e-4)

prior.train()
for epoch in range(parameters["n_epochs"]):
    train_loss = 0
    t_loss = 0
    val_loss = 0
    v_loss = 0

    for batch_idx, (x, label) in enumerate(train_loader):
        bs = x.size(0)
        vq_output_eval = model.pre_vq_conv(model.encoder(x.to(device)))
        _, _, _, x = model.vq_vae(vq_output_eval)
        x = (x[:, 0]).long()

        x = x.view(bs,8,8)
        x = x.to(device)
        label = label.to(device)

        logits = prior(x, label)
        logits = logits.permute(0, 2, 3, 1).contiguous()

        loss = criterion(
            logits.view(-1, parameters["num_embeddings"]),
            x.view(-1)
        )

        opt.zero_grad()
        loss.backward()
        opt.step()

        train_loss += loss.item()
    t_loss = train_loss/len(train_loader)

    with torch.no_grad():
        for batch_idx, (x, label) in enumerate(val_loader):
            bs = x.size(0)
            vq_output_eval = model.pre_vq_conv(model.encoder(x.to(device)))
            _, _, _, x = model.vq_vae(vq_output_eval)

            x = (x[:, 0]).long()
            x = x.view(bs, 8,8)
            label = label.to(device)

            logits = prior(x, label)


            logits = logits.permute(0, 2, 3, 1).contiguous()
            loss = criterion(
                logits.view(-1, parameters["num_embeddings"]),
                x.view(-1)
            )

            val_loss += (loss.item())

    v_loss = val_loss/len(val_loader)
    print('epoch: {}/{}, Train loss: {:.6f}, Validation loss: {:.6f}'.format(epoch+1, parameters['n_epochs'], t_loss, v_loss))

torch.save(prior.state_dict(), 'finalPixelCNN.pth')

prior.load_state_dict(torch.load('/kaggle/working/finalPixelCNN.pth'))
prior.eval()

def inference():
    x = int(input("Enter number of classes:\n"))
    y = int(input("Enter number of rows of generated images:\n"))
    label = torch.arange(x).expand(y,x).contiguous().view(-1)
    label = label.long().cuda()
    bs = x*y
    shape=(8,8)
    x_q = prior.generate(label, shape=(8,8), batch_size=bs)
    x_q = x_q.view(-1,1)
    encodings = torch.zeros(x_q.shape[0], parameters['num_embeddings']).to(device)
    encodings.scatter_(1, x_q, 1)

    embed_weight = model.vq_vae.embedding.weight
    quantized = torch.matmul(encodings, embed_weight).view(bs,*shape,-1)
    quantized = quantized.permute(0,3,1,2).contiguous()

    reconstructed = model.decoder(quantized)
    show(make_grid(reconstructed.detach().cpu()+0.5,nrow=7))
    print(label)

inference()